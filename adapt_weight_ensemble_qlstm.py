# -*- coding: utf-8 -*-
"""Copy_of_ensemble_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mPbOxZRiZdjsKO5wDgsfVvViqkEwIgGN
"""

!pip install pandas matplotlib torch

# Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader
import torch
from torch import nn
import helper
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error


import time
import numpy as np
import math
import matplotlib.pyplot as plt

import torch
from torch.utils.data import DataLoader
from torch import nn

from IPython.display import Image

def create_qlstm_model():
    Qmodel = QShallowRegressionLSTM(num_sensors=len(features), hidden_units=num_hidden_units, n_qubits=4)
    optimizer = torch.optim.Adagrad(Qmodel.parameters(), lr=learning_rate)
    loss_function = nn.MSELoss()
    return Qmodel, optimizer, loss_function

df = pd.read_csv('/content/harv.csv')
#df = df.drop(['Date' ,'Unnamed: 0'], axis=1)
df

target = "Temperature"

#features = ["Year","Quarter","Month","Week_Of_Year","Day_of_Year","Day_Of_Week","Stat_Hol","Hour_of_Day","Relative Humidity","Wind Speed","Visibility","Pressure","Wind_Chill","Dewpoint Temp"]
features = list(df.columns.difference(["Temperature"]))
features

size = int(len(df) * 0.87)
df_train = df.loc[:size].copy()
df_test = df.loc[size:].copy()

df_test

target_mean = df_train[target].mean()
target_stdev = df_train[target].std()

for c in df_train.columns:

    mean = df_train[c].mean()
    stdev = df_train[c].std()

    df_train[c] = (df_train[c] - mean) / stdev
    df_test[c] = (df_test[c] - mean) / stdev

!pip install pennylane

from Factory import SequenceDataset

torch.manual_seed(101)

batch_size = 300
sequence_length = 3

train_dataset = SequenceDataset(
    df_train,
    target=target,
    features=features,
    sequence_length=sequence_length
)
test_dataset = SequenceDataset(
    df_test,
    target=target,
    features=features,
    sequence_length=sequence_length
)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

X, y = next(iter(train_loader))

print("Features shape:", X.shape)
print("Target shape:", y.shape)

from Factory import QShallowRegressionLSTM

learning_rate = 0.0578278348
num_hidden_units = 5

Qmodel = QShallowRegressionLSTM(num_sensors=len(features), hidden_units=num_hidden_units, n_qubits=4)
loss_function = nn.MSELoss()
optimizer = torch.optim.Adagrad(Qmodel.parameters(), lr=learning_rate)

def train_model(data_loader, model, loss_function, optimizer):
    num_batches = len(data_loader)
    total_loss = 0
    model.train()

    for X, y in data_loader:
        output = model(X)
        loss = loss_function(output, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    avg_loss = total_loss / num_batches
    print(f"Train loss: {avg_loss}")
    return avg_loss

def test_model(data_loader, model, loss_function):

    num_batches = len(data_loader)
    total_loss = 0

    model.eval()
    with torch.no_grad():
        for X, y in data_loader:
            output = model(X)
            total_loss += loss_function(output, y).item()

    avg_loss = total_loss / num_batches
    print(f"Test loss (avg): {avg_loss}")
    # print(f"Test loss (total): {total_loss}")
    return avg_loss

def predict(data_loader, model):
    """Just like `test_loop` function but keep track of the outputs instead of the loss
    function.
    """
    output = torch.tensor([])
    model.eval()
    with torch.no_grad():
        for X, _ in data_loader:
            y_star = model(X)
            # print(f"y_star shape: {y_star.shape}")
            output = torch.cat((output, y_star), 0)
            # print(f"output: {output.shape}")

    return output

# Function to evaluate the model
def evaluate_model(true_values, predictions):
    mae = mean_absolute_error(true_values, predictions)
    mse = mean_squared_error(true_values, predictions)
    return mae, mse

def predict_ensemble(data_loader, models, weights, sequence_lengths, loss_function, batch_size=300):
    """Make predictions using an ensemble of models with varying sequence lengths and calculate loss."""
    predictions_list = torch.tensor([])
    total_loss = 0
    num_batches = len(data_loader)

    for model, seq_len in zip(models, sequence_lengths):
        # Set the sequence length for the current model
        data_loader.dataset.sequence_length = seq_len
        model.eval()
        #print(f"Using sequence length {seq_len} for model: {model}")

    with torch.no_grad():
        for batch_idx, (X, y) in enumerate(data_loader):
            model_outputs = torch.stack([model(X[:, :seq_len]) for model, seq_len in zip(models, sequence_lengths)], dim=0)
            #print(f"model output shape : {model_outputs.shape}");
            weighted_outputs = model_outputs * weights.unsqueeze(0).unsqueeze(-1)
            #print(f"weighted_outputs shape : {weighted_outputs.shape}");
            weighted_avg_output = torch.sum(weighted_outputs, dim=1) / torch.sum(weights)
            #print(f"weighted_avg_outputs shape : {weighted_avg_output.shape}");

            # Print the first 10 model outputs in a structured format
            # if batch_idx == 0:
            #   for model_idx, model_output in enumerate(model_outputs):
            #       print(f"Model {model_idx + 1} outputs:")
            #       for output_idx, value in enumerate(model_output[:5]):
            #           print(f"  Output {output_idx + 1}: {value}")
            #   print("Weighted avg outputs:")
            #   for output_idx, value in enumerate(weighted_avg_output.squeeze()[:5]):
            #       print(f"  Output {output_idx + 1}: {value}")
              # print(f"shape of weighted_avf output: {(weighted_avg_output.squeeze()).shape}")
              # print(f"shape of y: {y.shape}")


            # Calculate loss for the current batch
            batch_loss = loss_function(weighted_avg_output.squeeze(), y)
            total_loss += batch_loss.item()

             # Append the predictions to the list
            # print(f"Shape of prediction_list = {predictions_list.shape}")
            predictions_list = torch.cat((predictions_list, weighted_avg_output.squeeze()), 0)

            # Release GPU memory periodically
            if batch_idx % (batch_size // 2) == 0:
                torch.cuda.empty_cache()

    # Calculate average loss over all batches
    avg_loss = total_loss / num_batches
    print(f"Ensemble Test Loss: {avg_loss}")

    return predictions_list, avg_loss



def compute_combining_weights(errors, current_epoch, forgetting_factor=0.85):
    num_models, num_epochs = errors.shape
    weights = np.zeros_like(errors)
    final_weights = np.zeros(num_models)

    if(current_epoch == 0):

        # Normalize weights for each model at each epoch
        weights[:, 0] = 1 / errors[:, 0]
        #print(weights)



    for t in range(1, current_epoch + 1):

        # Calculate the past prediction error for each model
        # print(f"sum of errors = {np.sum(errors[:, :t], axis=1)}")
        past_prediction_errors = 1 / np.sum(errors[:, :t], axis=1)
        # print(f"past pred array = {past_prediction_errors}")

        # Update weights for each model
        weights[:, t] = (1 + forgetting_factor * past_prediction_errors) / np.sum(past_prediction_errors)
        #print(f"Before normalising Epoch {t} weights: {weights[t, :]}")

        # Normalize weights for each model at each epoch
        # weights[:, t] = 1 / weights[:, t]

   # print(f"sum of weights column wise: {np.sum(weights[:, :current_epoch + 1], axis=0, keepdims=True)}")
    # Normalize weights across models at each epoch
    weights_normalized = weights[:, :current_epoch + 1] / np.sum(weights[:, :current_epoch + 1], axis=0, keepdims=True)
    # print(f"After normalising Epoch {t} weights: {weights[t, :]}")
    final_weights = weights_normalized[:, -1]
    #print(f"Final weights: {final_weights}")

    return final_weights

# Train the QLSTM ensemble
sequence_lengths = [3, 5, 7, 9]  # Example sequence lengths
num_ensemble_models = 4 # Example number of ensemble models
epochs = 10  # Example number of epochs
forgetting_factor = 0.85 #Example forgetting factor

optimised_hidden_units = [5, 5, 5, 5]
optimised_n_qubits = [4, 4, 4, 4]
optimised_learning_rate = [0.06770657553, 0.06911686436, 0.069225405114, 0.06922160798356]
optimised_batch_size = [66, 166, 160, 160]

# Initialize weights and errors outside the loop
weights = np.ones((num_ensemble_models,))
errors = np.zeros((num_ensemble_models, epochs))


# Create a new QLSTM model for the current sequence length
Qmodel1 = QShallowRegressionLSTM(num_sensors=len(features), hidden_units=optimised_hidden_units[0], n_qubits=optimised_n_qubits[0])
Qmodel2 = QShallowRegressionLSTM(num_sensors=len(features), hidden_units=optimised_hidden_units[1], n_qubits=optimised_n_qubits[1])
Qmodel3 = QShallowRegressionLSTM(num_sensors=len(features), hidden_units=optimised_hidden_units[2], n_qubits=optimised_n_qubits[2])
Qmodel4 = QShallowRegressionLSTM(num_sensors=len(features), hidden_units=optimised_hidden_units[3], n_qubits=optimised_n_qubits[3])

# store all qmodels
Qmodels = [Qmodel1, Qmodel2, Qmodel3, Qmodel4]

optimizer1 = torch.optim.Adagrad(Qmodel1.parameters(), lr=optimised_learning_rate[0])
optimizer2 = torch.optim.Adagrad(Qmodel2.parameters(), lr=optimised_learning_rate[1])
optimizer3 = torch.optim.Adagrad(Qmodel3.parameters(), lr=optimised_learning_rate[2])
optimizer4 = torch.optim.Adagrad(Qmodel4.parameters(), lr=optimised_learning_rate[3])

optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]



# Train the QLSTMs simultaneously for each timestamp
for epoch in range(epochs):
    print(f"\nEpoch {epoch + 1}\n{'='*20}")

    # Create an empty list to store ensemble models for the current epoch
    ensemble_models_epoch = []

    # Iterate over different sequence lengths
    for i, seq_len in enumerate(sequence_lengths):
        print(f"Training QLSTM with sequence length {seq_len}\n{'-'*40}")

        # Modify the sequence length in the dataset
        train_dataset = SequenceDataset(
            df_train,
            target=target,
            features=features,
            sequence_length=seq_len
        )

        # Recreate data loader for training
        train_loader = DataLoader(train_dataset, batch_size=optimised_batch_size[i], shuffle=True, num_workers=2)

        test_dataset = SequenceDataset(
            df_test,
            target=target,
            features=features,
            sequence_length=seq_len  # You can choose any sequence length here
        )
        test_loader = DataLoader(test_dataset, batch_size=optimised_batch_size[i], shuffle=True, num_workers=2)

        # Train the model for 1 epoch
        train_loss = train_model(train_loader, Qmodels[i], loss_function, optimizer=optimizers[i])

        # Evaluate the QLSTM model on the test set
        test_loss = test_model(test_loader, Qmodels[i], loss_function)

        # Update errors based on test loss
        errors[i, epoch] = test_loss

        # Clear variables to free up memory
        del train_dataset, train_loader, train_loss
        torch.cuda.empty_cache()  #release GPU memory

    # Compute combining weights using the recursive formula for the current epoch
    weights = compute_combining_weights(errors, epoch)
    # Print the weights if needed
    print(f"Combining weights: {weights}")

    # Convert weights to PyTorch tensor
    weights_tensor = torch.tensor(weights)

    # Make predictions using the ensemble_predict function for the current epoch
    with torch.no_grad():
        # ensemble_predictions = predict_ensemble(test_loader, Qmodels, weights_tensor).flatten()
        ensemble_predictions, avg_loss = predict_ensemble(test_loader, Qmodels, weights_tensor, sequence_lengths, loss_function)
    print(f"Ensemble error:{avg_loss}")

    print("---------------1----------------")
    ground_truth_values = df_test[target].values


    # Clear variables to free up memory
    del ensemble_models_epoch, weights_tensor, ensemble_predictions, avg_loss
    torch.cuda.empty_cache()  # releasing GPU Memory

# Print the final ensemble error if needed
# final_avg_ensemble_error = np.mean(errors)
# print(f"\nFinal Ensemble error: {final_avg_ensemble_error}")

target_mean = df_train[target].mean()
target_stdev = df_train[target].std()

# Modify the sequence length in the dataset
train_dataset = SequenceDataset(
    df_train,
    target=target,
    features=features
)
train_loader = DataLoader(train_dataset, batch_size=300, shuffle=True, num_workers=2)
test_dataset = SequenceDataset(
            df_test,
            target=target,
            features=features
        )
test_loader = DataLoader(test_dataset, batch_size=300, shuffle=True, num_workers=2)


train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)

# Convert weights to PyTorch tensor
weights_tensor = torch.tensor(weights)

ystar_col = "Model forecast"
train_prediction, train_loss = predict_ensemble(train_loader, Qmodels, weights_tensor, sequence_lengths, loss_function)
df_train[ystar_col] = train_prediction.numpy()
test_prediction, test_loss = predict_ensemble(test_loader, Qmodels, weights_tensor, sequence_lengths, loss_function)
df_test[ystar_col] =  test_prediction.numpy()
df_out = pd.concat((df_train, df_test))[[target, ystar_col]]

# If you have target_mean and target_stdev defined
for c in df_out.columns:
    df_out[c] = df_out[c] * target_stdev + target_mean

print(df_out)

for c in df_out.columns:
    df_out[c] = df_out[c] * target_stdev + target_mean

plt.figure(figsize=(12, 7))
n = 1  # Plot every 5th data point
plt.plot(range(96433) [::n], df_out["Temperature"][::n], label = "Real")
plt.plot(range(96433)[::n], df_out["Model forecast"][::n], label = "Ensemble Prediction")
plt.ylabel('temperature')
plt.xlabel('Days')
# Set x-axis limits to zoom into a specific region
start_index = 1000
end_index = 1500
plt.xlim(start_index, end_index)
plt.vlines(size, ymin = -10, ymax = 10, label = "Test set start", linestyles = "dashed")
plt.legend()
plt.show()

# Modify the sequence length in the dataset
train_dataset = SequenceDataset(
    df_train,
    target=target,
    features=features,
    sequence_length=3

)
train_loader = DataLoader(train_dataset, batch_size=300, shuffle=True, num_workers=2)
test_dataset = SequenceDataset(
            df_test,
            target=target,
            features=features,
            sequence_length=3
        )
test_loader = DataLoader(test_dataset, batch_size=300, shuffle=True, num_workers=2)


train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)

# Convert weights to PyTorch tensor
weights_tensor = torch.tensor(weights)

ystar_col = "Model forecast"
df_train[ystar_col] = predict(train_eval_loader, Qmodels[0]).numpy()
df_test[ystar_col] = predict(test_loader, Qmodels[0]).numpy()

df_out = pd.concat((df_train, df_test))[[target, ystar_col]]

# # If you have target_mean and target_stdev defined
# for c in df_out.columns:
#     df_out[c] = df_out[c] * target_stdev + target_mean

print(df_out)

plt.figure(figsize=(12, 7))
n = 1  # Plot every 5th data point
plt.plot(range(96433) [::n], df_out["Temperature"][::n], label = "Real")
plt.plot(range(96433)[::n], df_out["Model forecast"][::n], label = "LSTM1 Prediction")
plt.ylabel('temperature')
plt.xlabel('Days')
# Set x-axis limits to zoom into a specific region
start_index = 1000
end_index = 1500
plt.xlim(start_index, end_index)
plt.vlines(size, ymin = -10, ymax = 10, label = "Test set start", linestyles = "dashed")
plt.legend()
plt.show()

# Modify the sequence length in the dataset
train_dataset = SequenceDataset(
    df_train,
    target=target,
    features=features,
    sequence_length=5

)
train_loader = DataLoader(train_dataset, batch_size=300, shuffle=True, num_workers=2)
test_dataset = SequenceDataset(
            df_test,
            target=target,
            features=features,
            sequence_length=3
        )
test_loader = DataLoader(test_dataset, batch_size=300, shuffle=True, num_workers=2)


train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)

# Convert weights to PyTorch tensor
weights_tensor = torch.tensor(weights)

ystar_col = "Model forecast"
df_train[ystar_col] = predict(train_eval_loader, Qmodels[1]).numpy()
df_test[ystar_col] = predict(test_loader, Qmodels[1]).numpy()

df_out = pd.concat((df_train, df_test))[[target, ystar_col]]

# # If you have target_mean and target_stdev defined
# for c in df_out.columns:
#     df_out[c] = df_out[c] * target_stdev + target_mean

print(df_out)

plt.figure(figsize=(12, 7))
n = 1  # Plot every 5th data point
plt.plot(range(96433) [::n], df_out["Temperature"][::n], label = "Real")
plt.plot(range(96433)[::n], df_out["Model forecast"][::n], label = "LSTM2 Prediction")
plt.ylabel('temperature')
plt.xlabel('Days')
# Set x-axis limits to zoom into a specific region
start_index = 1000
end_index = 1500
plt.xlim(start_index, end_index)
plt.vlines(size, ymin = -10, ymax = 10, label = "Test set start", linestyles = "dashed")
plt.legend()
plt.show()

# Modify the sequence length in the dataset
train_dataset = SequenceDataset(
    df_train,
    target=target,
    features=features,
    sequence_length=7

)
train_loader = DataLoader(train_dataset, batch_size=300, shuffle=True, num_workers=2)
test_dataset = SequenceDataset(
            df_test,
            target=target,
            features=features,
            sequence_length=3
        )
test_loader = DataLoader(test_dataset, batch_size=300, shuffle=True, num_workers=2)


train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)

# Convert weights to PyTorch tensor
weights_tensor = torch.tensor(weights)

ystar_col = "Model forecast"
df_train[ystar_col] = predict(train_eval_loader, Qmodels[2]).numpy()
df_test[ystar_col] = predict(test_loader, Qmodels[2]).numpy()

df_out = pd.concat((df_train, df_test))[[target, ystar_col]]

# # If you have target_mean and target_stdev defined
# for c in df_out.columns:
#     df_out[c] = df_out[c] * target_stdev + target_mean

print(df_out)

plt.figure(figsize=(12, 7))
n = 1  # Plot every 5th data point
plt.plot(range(96433) [::n], df_out["Temperature"][::n], label = "Real")
plt.plot(range(96433)[::n], df_out["Model forecast"][::n], label = "LSTM3 Prediction")
plt.ylabel('temperature')
plt.xlabel('Days')
# Set x-axis limits to zoom into a specific region
start_index = 1000
end_index = 1500
plt.xlim(start_index, end_index)
plt.vlines(size, ymin = -10, ymax = 10, label = "Test set start", linestyles = "dashed")
plt.legend()
plt.show()

# Modify the sequence length in the dataset
train_dataset = SequenceDataset(
    df_train,
    target=target,
    features=features,
    sequence_length=9

)
train_loader = DataLoader(train_dataset, batch_size=300, shuffle=True, num_workers=2)
test_dataset = SequenceDataset(
            df_test,
            target=target,
            features=features,
            sequence_length=3
        )
test_loader = DataLoader(test_dataset, batch_size=300, shuffle=True, num_workers=2)


train_eval_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)

# Convert weights to PyTorch tensor
weights_tensor = torch.tensor(weights)

ystar_col = "Model forecast"
df_train[ystar_col] = predict(train_eval_loader, Qmodels[3]).numpy()
df_test[ystar_col] = predict(test_loader, Qmodels[3]).numpy()

df_out = pd.concat((df_train, df_test))[[target, ystar_col]]

# # If you have target_mean and target_stdev defined
# for c in df_out.columns:
#     df_out[c] = df_out[c] * target_stdev + target_mean

print(df_out)

plt.figure(figsize=(12, 7))
n = 1  # Plot every 5th data point
plt.plot(range(96433) [::n], df_out["Temperature"][::n], label = "Real")
plt.plot(range(96433)[::n], df_out["Model forecast"][::n], label = "LSTM4 Prediction")
plt.ylabel('temperature')
plt.xlabel('Days')
# Set x-axis limits to zoom into a specific region
start_index = 1000
end_index = 1500
plt.xlim(start_index, end_index)
plt.vlines(size, ymin = -10, ymax = 10, label = "Test set start", linestyles = "dashed")
plt.legend()
plt.show()